### De ce este necesară masca?

Masca este folosită pentru a restricționa informația disponibilă unui token în funcție de tipul modelului:

- În **modelele auto-regresive** (ex: GPT), dorim ca fiecare token să vadă **doar trecutul**.
- În **modelele bidirecționale** (ex: BERT), fiecare token poate vedea **întreaga secvență**.

### Masked Self-Attention

În modelele auto-regresive, este esențial ca un token să **nu aibă acces la viitor**, pentru a învăța să prezică următorul cuvânt fără să trișeze.

Se folosește o **mască triunghiulară inferioară**, unde pozițiile viitoare sunt blocate:

$\text{Mask}[i][j] = \begin{cases} 0 & \text{dacă } j \leq i \quad (\text{vizibil}) \\\\ -\infty & \text{dacă } j > i \quad (\text{mascat}) \end{cases}$

Această mască este adunată la scorurile de atenție înainte de aplicarea funcției softmax.


### Full Self-Attention

În modelele precum BERT, care sunt bidirecționale, **nu se folosește mască**. Fiecare token poate interacționa cu toate celelalte:

- Este potrivit pentru sarcini precum clasificarea de text, analiză de sentiment, întrebări și răspunsuri etc.
- Contextul complet al propoziției este disponibil pentru fiecare poziție.

### Exemplu

Pentru secvența: `Eu mănânc mere roșii`

- În GPT (cu mască):  
    Când modelul procesează tokenul „mere”, poate vedea doar `Eu`, `mănânc`.
- În BERT (fără mască):  
    Tokenul „mănânc” poate vedea atât `Eu`, cât și `mere`, `roșii`.

### Rezumat

|Model|Tip atenție|Poate vedea viitorul?|
|---|---|---|
|GPT|Masked Self-Attention|Nu|
|BERT|Full Self-Attention|Da|
