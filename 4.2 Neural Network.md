[YouTube][https://youtu.be/aircAruvnKk]

==Neural networks are a type of machine learning algorithm==, but they differ from traditional machine learning in several key ways. Most importantly, neural networks learn and improve on their own, **without human intervention**. It learns **features directly from the data**, making it better suited for large datasets. However, in traditional machine learning, features are manually provided.

### Why to use Deep Learning?

One of the key benefits of deep learning is its ability to handle big data. As the volume of data increases, traditional machine learning techniques can become inefficient in terms of performance and accuracy. Deep learning, on the other hand, continues to perform well, making it an ideal choice for data-heavy applications.

![[Pasted image 20250512121415.png]]
### Benefits of understanding the structure?

By analyzing the structure of a neural network, we can identify ways to optimize it for better performance. For example, we can adjust the number of layers or nodes, or **tweak** the way the network processes input data. we can also we can **develop neural networks** that are better suited for analyzing medical images, or for predicting the stock market. If we know which nodes in the network are activated for a particular input, we can better **understand how the network arrived at its decision** or prediction.

### How Neural Network works?

Each neuron represents a unit of computation that takes in a set of inputs, performs a set of calculations, and produces an output that is passed on to the next layer.

Just like the neurons in our brains, **each node in a neural network receives input, processes it, and passes the output on to the next node. As the data moves through the network, the connections between the nodes are strengthened or weakened, depending on the patterns in the data. This allows the network to learn from the data and make predictions or decisions based on what it has learned.**

Imagine a 28 by 28 grid, where a number is drawn in such a way that some pixels are darker than others. By identifying the brighter pixels, we can decipher the number that was written on the grid. This grid serves as the input for a neural network.

![[Pasted image 20250512121524.png]]

The rows of the grid are arranged in a horizontal 1-D array, which is then transformed into a vertical array, forming the first layer of neurons. Just like this;

![[Pasted image 20250512121628.png]]![[Pasted image 20250512121704.png]]

In the case of the first layer, each neuron corresponds to a single pixel in the input image, and the value inside each neuron represents the **activation** or intensity of that pixel. The input layer of a neural network is responsible for taking in the raw data (in this case, an image) and transforming it into a format that can be processed by the rest of the network.

In this case, we have 28x28 input pixels, which gives us a total of 784 neurons in the input layer. Each neuron will have an activation value of either 0 or 1, depending on whether the corresponding pixel in the input image is black or white, respectively. 
*Number inside the neuron: greyscale for the pixel*

![[Pasted image 20250512121841.png]]

**The output layer of the neural network** consists of 10 neurons in this case, each of which represents a possible output class (in this case, the digits 0 through 9). The output of each neuron in the output layer represents the probability that the input image belongs to that particular class. The highest probability value determines the predicted class for that input image.


### Hidden Layers

In between the input and output layers, we have one or more hidden layers, **which perform a series of non-linear transformations** on the input data. The purpose of these hidden layers is to extract higher-level features from the input data that are more meaningful for the task at hand. It is upto you how many hidden layers you want to add in your network.

![[Pasted image 20250512121941.png]]Each neuron in the hidden layer receives inputs from all neurons in the previous layer, and applies a set of weights and biases to those inputs before passing the result through a non-linear activation function. This process is repeated across all neurons in the hidden layer until the output layer is reached.


### Forward Propagation

Forward propagation is the process by which input data is passed through a neural network to generate an output. It involves computing the output of each neuron in each layer of the network by applying the weights and biases to the inputs and passing the results through an activation function.

Equation:
$y = g (w_o + Sigma x_i. w_i)$ or $y = g (w_o + Xt. W)$


where **y** is the output of the neural network, **g** is the non-linear activation function, **xi** refers to the i-th input feature or input variable, **wi** is the weight associated with the i-th input feature or variabl, and **wo** is the bias term, which is a constant value that is added to the linear combination of inputs.

**Sigma xi. wi:** This is the linear combination of the input features and their associated weights. This term is also sometimes referred to as the “weighted sum” of the inputs.

### Backpropagation

![[Pasted image 20250512124032.png]]![[Pasted image 20250512124055.png]]

Backpropagation is a popular algorithm used in training neural networks. It involves calculating the **gradient**, which is **a measure of the change in the loss function** with respect to each weight in the network. The **loss function is a measure of how well the neural network is able to predict the correct output for a given input**. By calculating the gradient of the loss function, backpropagation allows the neural network to update its weights in a way that reduces the overall error or loss during training.

The algorithm works by propagating the error from the output layer back through the layers of the network, using the chain rule of calculus to calculate the gradient of the loss function with respect to each weight. This gradient is then used in gradient descent optimization to update the weights and minimize the loss function.

## **Terminologies used in Neural Networks**

- **Training of the neural network** is the process of adjusting the weights of a neural network based on input data and desired output, in order to improve the accuracy of the network’s predictions.
- **Weight:** weights refer to the parameters that are learned during training, and they determine the strength of the connections between neurons. Each connection between neurons is assigned a weight, which is multiplied by the input value to the neuron to determine its output.
- **Bias:** Bias is another learned parameter that is added to the weighted sum of inputs to a neuron in a given layer. It is an additional input to the neuron that helps to adjust the output of the activation function.
- **Non-linear activation function:** A non-linear activation function is applied to the output of a neuron to introduce non-linearity into the network. Non-linearity is important because it allows the network to model complex, nonlinear relationships between inputs and outputs. Common activation functions used in neural networks include the sigmoid function, the ReLU (Rectified Linear Unit) function, and the softmax function.
- **Loss function:** This is a mathematical function that measures the error or difference between the predicted output of the neural network and the true output. The _empirical loss_ measures the total loss over our entire dataset. _Cross-entropy loss_ is commonly used with models that output a probability between 0 and 1, while _mean squared error loss_ is used with regression models that output continuous real numbers. The goal is to minimize the loss function during training in order to improve the accuracy of the network’s predictions.
- **Loss optimization:** This is the process of minimizing the error or loss incurred by the neural network when making predictions. This is done by adjusting the weights of the network.
- **Gradient descent:** This is an optimization algorithm used to find the minimum of a function, such as the loss function of a neural network. It involves iteratively adjusting the weights in the direction of the negative gradient of the loss function. The idea is to keep moving the weights in the direction that reduces the loss, until we reach the minimum.
- ![[Pasted image 20250512124225.png]]
- 