**Regula lanțului** este o regulă din calculul diferențial care ne permite să calculăm derivata unei funcții **compuse**.
Dacă o variabilă $z$ depinde de $y$, iar $y$ depinde de $x$, atunci:

$\frac{dz}{dx} = \frac{dz}{dy} \cdot \frac{dy}{dx}$

Este o metodă de **a propaga modificările** înapoi printr-un lanț de funcții.

#### Exemplu simplu

Să zicem că avem:

- $x$ este o variabilă de intrare
- $y = f(x)$
- $z = g(y)$

Atunci, $z = g(f(x))$, adică o compunere de funcții.

Derivata lui $z$ în raport cu $x$ este:

$\frac{dz}{dx} = \frac{dg}{dy} \cdot \frac{df}{dx}$


#### Aplicație în rețele neuronale

Într-o rețea neuronală, ieșirea unui neuron este:

$\hat{y} = f(z), \quad \text{unde} \quad z = \sum w_i x_i + b$

Iar eroarea totală este o funcție a ieșirii: $E = L(\hat{y}, y)$.

Pentru a ajusta greutatea $w_j$, trebuie să calculăm:

$\frac{\partial E}{\partial w_j}$

Folosim regula lanțului pentru a împărți acest calcul în pași:

$\frac{\partial E}{\partial w_j} = \frac{\partial E}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} \cdot \frac{\partial z}{\partial w_j}$

Fiecare componentă este:
- $\frac{\partial E}{\partial \hat{y}}$ = cum afectează schimbarea ieșirii eroarea
- $\frac{\partial \hat{y}}{\partial z}$ = derivata funcției de activare
- $\frac{\partial z}{\partial w_j} = x_j$ = intrarea corespunzătoare acelei greutăți

#### Intuiție

Regula lanțului te ajută să înțelegi **cum o mică schimbare într-un parametru de la începutul rețelei afectează eroarea finală**.

Este esențială pentru **backpropagation**, pentru că rețelele neuronale sunt compuse din multe funcții una peste alta — exact cazul pentru care se aplică regula lanțului.

#### Vizualizare

Imaginează-ți o serie de rotițe conectate. Dacă rotiți ultima rotiță, efectul se transmite înapoi prin toate celelalte. Regula lanțului spune cât de mult trebuie să rotești fiecare rotiță ca să obții un anumit efect la final.
