
**Transformerii** sunt o arhitectură de rețea neuronală introdusă în lucrarea _"Attention is All You Need"_ (2017), care a revoluționat procesarea limbajului natural (NLP). Sunt baza modelelor LLM precum GPT, BERT, T5 etc.

#### Scopul principal

Transformerii sunt concepuți pentru a înțelege și a genera secvențe (texte, cod etc.) **fără a parcurge datele secvențial**, ci în paralel, folosind un mecanism numit **Self-Attention**.

#### Componente cheie ale unui Transformer

##### 1. Input Embeddings

Fiecare cuvânt este transformat într-un vector numeric (embedding), care capturează semnificația sa.

Exemplu: „chat” → [0.2, -0.5, 0.7, …]

**Embeddings statice** (ex: Word2Vec, GloVe):

- Cuvântul `cat` are **același vector embedding** indiferent de context.
- Exemplu:
    - `cat` în „The cat is sleeping.” și `cat` în „I love cat memes.” → **același vector**

Limitare: Nu distinge între sensuri diferite (ex: "bank" ca instituție vs "bank" ca mal de râu)

**Embeddings contextuale** (ex: BERT, GPT):

- `cat` are **embedding diferit** în funcție de contextul în care apare.
- Exemplu:
    - `cat` în „The cat sat on the mat.” vs `cat` în „He is a cat burglar.” → **embedding-uri diferite**
    
Avantaj: Poate distinge sensuri, nuanțe, intenții, ironii etc.

##### 2. Positional Encoding

Problema: Transformerele **nu au recurență** și nici structură secvențială implicită. Tratează toate cuvintele simultan.

Dar ordinea cuvintelor contează:  
„The dog bit the man” ≠ „The man bit the dog”

**Soluție:** Positional Encoding (PE) — se adaugă un vector de poziție fiecărui cuvânt:

```
Embedding final = WordEmbedding + PositionalEncoding
```

Formule:

- $PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)$
- $PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)$

unde $d$ este dimensiunea embeddingului.

##### 3. Self-Attention

**Self-Attention** este mecanismul prin care modelul decide care cuvinte sunt importante unele pentru altele.

Fiecare cuvânt este transformat în:

- Query ($Q$)
- Key ($K$)
- Value ($V$)

Formula:

$Attention(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$

Se calculează scoruri între Query și Key, se normalizează cu softmax și se aplică ponderi la Value.

##### 4. Multi-Head Attention

În loc de un singur mecanism de atenție, se folosesc mai multe în paralel (capete multiple). Fiecare cap învață relații diferite între cuvinte.

Formule:

- $Q_i = XW_i^Q$
- $K_i = XW_i^K$
- $V_i = XW_i^V$
- $head_i = Attention(Q_i, K_i, V_i)$
- $MultiHead(Q, K, V) = \text{Concat}(head_1, ..., head_h)W^O$

##### 5. Feed Forward Neural Network (FFN)

Este o rețea complet conectată aplicată fiecărui vector de cuvânt:

$FFN(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2$

Transformă vectorul fiecărui cuvânt într-o reprezentare mai abstractă, fără a combina cuvinte între ele.

##### 6. Skip Connections + Normalizare

**Skip Connections**: permit transmiterea informației și a gradientului între straturi pentru stabilitate și învățare mai rapidă.

**Normalizare** (ex: LayerNorm, BatchNorm): stabilizează activările neuronilor și accelerează procesul de antrenare.

#### Structura completă

- **Encoder**: primește textul și îl transformă în vectori.
- **Decoder**: generează textul de ieșire pe baza vectorilor.

GPT folosește doar decodere. BERT doar encodere.

#### Avantaje ale Transformerilor

- Permite antrenare paralelă
- Captează relații pe termen lung
- Scalabil la seturi mari de date
