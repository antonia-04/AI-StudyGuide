
#### Scopul metodei celor mai mici pătrate (Least Squares)

**Scopul:**  
Să învățăm o **funcție liniară** care aproximează cât mai bine relația dintre un vector de intrare $x∈Rd\mathbf{x} \in \mathbb{R}^d$ și o valoare de ieșire $y∈Ry \in \mathbb{R}$, pe baza unor **exemple etichetate** (învățare supervizată).

Acest model este folosit în special în probleme de **regresie**, unde dorim să **prezicem o valoare numerică continuă**.


#### Definirea problemei de regresie

Presupunem un set de date format din:
- Vectori de intrare:  
    $\mathbf {x}_i \in \mathbb{R}^d, pentru \ i=1,2,...,n$
- Valori de ieșire (etichetă):  
    $y_i \in \mathbb{R}$
    
Dorim să găsim un **model liniar** ff, astfel încât pentru fiecare exemplu ii, valoarea prezisă $f(\mathbf{x}_i)$ să fie cât mai apropiată de valoarea reală $y_i$.



### Modelul Liniar

Funcția liniară are forma:

$f(\mathbf{x}) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_d x_d$

Putem rescrie acest model folosind notație vectorială:

- $\mathbf{x} = (1, x_1, x_2, ..., x_d)^T \in \mathbb{R}^{d+1}$ — adăugăm un 1 pentru bias/intercept
- $\boldsymbol{\beta} = (\beta_0, \beta_1, ..., \beta_d)^T \in \mathbb{R}^{d+1}$

Astfel:

$f(\mathbf{x}) = \mathbf{x}^T \boldsymbol{\beta}$


#####  Funcția de cost (Loss function)

Pentru a măsura **cât de bine** se potrivește modelul pe datele existente, folosim o **funcție de cost** care măsoară **eroarea totală** dintre valorile reale și cele prezise:

$\text{Loss}(\boldsymbol{\beta}) = \sum_{i=1}^{n} (y_i - f(\mathbf{x}_i))^2$

Aceasta este cunoscută ca **suma pătratelor erorilor** (Sum of Squared Errors – SSE).

Reprezentată vectorial:

- $\mathbf{y} = (y_1, y_2, ..., y_n)^T$
- $X \in \mathbb{R}^{n \times (d+1)}$ este matricea design-ului (fiecare rând este un $xi$)

Atunci funcția de cost devine:

${Loss}(\boldsymbol{\beta}) = \| \mathbf{y} - X\boldsymbol{\beta} \|^2$


##### Soluția optimă

Pentru a minimiza această funcție de cost, derivăm în raport cu β, egalăm cu 0 și obținem:

${\beta} = (X^T X)^{-1} X^T \mathbf{y}$

Aceasta este formula de învățare (estimare a coeficienților) pentru regresia liniară folosind metoda celor mai mici pătrate.

#### Cazul simplu: regresie cu o singură variabilă (d=1d = 1)

Dacă avem o singură variabilă de intrare $x$ $\in \mathbb{R},$ atunci:

$f(x) = \beta_0 + \beta_1 x$

Coeficienții pot fi calculați direct:

$beta_1 = \frac{\text{cov}(x, y)}{\text{var}(x)}$
$β0=yˉ−β1xˉ$

Unde:

- $\bar{x}, \bar{y}$ sunt mediile variabilelor
- ${cov}(x, y)$ = covarianța
- ${var}(x)$ = varianța


#### Vizualizări importante

##### Anscombe's Quartet

- Patru seturi de date cu **aceiași coeficienți de regresie**, **medii** și **varianțe**, dar **distribuții foarte diferite**.
- Ilustrează faptul că e **esențial să vizualizăm datele** – regresia liniară poate fi înșelătoare în lipsa contextului grafic.
- Anscombe's Quartet este un set de **patru seturi de date** care au aceleași statistici descriptive (media, varianța, corelația), dar comportamente diferite în grafice. Acesta subliniază importanța **vizualizării datelor**, pentru că statistica poate fi înșelătoare. Scopul este să arăți că **datele vizualizate** pot dezvălui relații non-liniare sau **outlieri** care nu sunt evidente doar din analize statistice.
![[Pasted image 20250511203955.png]]

##### Graficul reziduurilor (Residual Plot)

- Afișează diferențele $y_i - f(x_i)$ pentru fiecare punct.
- Un **grafic al reziduurilor** este folosit pentru a evalua performanța unui model de regresie. Reziduurile sunt diferențele dintre valorile reale și cele prezise. Într-un grafic al reziduurilor:
	- **Reziduuri aleatorii** indică un model bun.
	- **Pattern curbat** sugerează că modelul nu este adecvat (poate fi necesar un model non-liniar).
	- **Outlieri** indică date care influențează puternic modelul.
	- **Heteroscedasticitate** apare când variabilitatea reziduurilor crește odată cu valorile prezise.
Scopul graficului de reziduuri este să ajute la **diagnosticarea** și îmbunătățirea modelului de regresie.
