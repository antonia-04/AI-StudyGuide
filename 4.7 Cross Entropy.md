![[Pasted image 20250610211158.png]]


**Cross entropy** este o **funcție de cost** utilizată frecvent în problemele de clasificare, în special împreună cu funcția **softmax**, pentru a măsura **cât de aproape este distribuția prezisă de distribuția reală (eticheta)**.

#### Formula – un singur exemplu

Fie $y = [0,\ 1,\ 0]$ eticheta corectă (one-hot encoded), și $\hat{y} = [0.2,\ 0.7,\ 0.1]$ vectorul de probabilități prezis de model.

Funcția de cross entropy este:

$E = -\sum_{i=1}^{K} y_i \log(\hat{y}_i)$

unde $K$ este numărul total de clase.

În exemplul de mai sus, doar $y_2 = 1$, deci:

$E = -\log(0.7) \approx 0.357$


#### Formula – întregul set de date (pentru $m$ exemple)

$E = -\frac{1}{m} \sum_{i=1}^{m} \sum_{j=1}^{K} y_j^{(i)} \log(\hat{y}_j^{(i)})$

unde:

- $y_j^{(i)}$ este eticheta corectă pentru exemplul $i$ și clasa $j$
- $\hat{y}_j^{(i)}$ este probabilitatea prezisă de model

#### Intuiție

- Dacă modelul prezice o probabilitate **mare** pentru clasa corectă → $E$ este **mic**
- Dacă modelul prezice o probabilitate **mică** pentru clasa corectă → $E$ este **mare**

Scopul rețelei este să **minimizeze această funcție de cost**, atribuind o probabilitate cât mai mare clasei corecte.

#### Comparativ: Cross Entropy vs MSE

|Funcție de cost|Potrivit pentru|Observații|
|---|---|---|
|MSE (Mean Squared Error)|regresie|derivata devine mică la softmax → învățare lentă|
|Cross Entropy|clasificare|gradientul rămâne mare chiar și pentru erori mici|
