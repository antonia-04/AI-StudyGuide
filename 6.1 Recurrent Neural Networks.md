
**RNN-urile** sunt un tip special de rețele neuronale care pot procesa **date secvențiale**, adică informații care vin în ordine (ex: propoziții, înregistrări audio, serii temporale).


#### De ce nu e suficient un MLP?

Rețelele neuronale obișnuite (MLP) nu au „memorie” și tratează fiecare intrare independent. Însă în multe sarcini, **ordinea elementelor contează**.

Exemple:

- Propoziție: „Ion a mâncat o prăjitură.”
- Dacă ordinea cuvintelor se schimbă → înțelesul se schimbă.


#### Ideea principală din RNN

O RNN parcurge datele **pas cu pas**, păstrând o **stare internă** (numită _hidden state_), care este actualizată la fiecare pas pe baza:

- valorii curente de intrare
- stării anterioare

Astfel, RNN-ul poate „ține minte” contextul anterior.


#### Formule pentru RNN

Presupunem că avem:

- $\mathbf{x}_t$ = vectorul de intrare la momentul $t$
- $\mathbf{h}_t$ = starea ascunsă (hidden state) la momentul $t$
- $\mathbf{y}_t$ = ieșirea rețelei la momentul $t$

Actualizarea stării:

$\mathbf{h}_t = \tanh(\mathbf{W}_{hx} \cdot \mathbf{x}_t + \mathbf{W}_{hh} \cdot \mathbf{h}_{t-1} + \mathbf{b}_h)$

Ieșirea:

$\mathbf{y}_t = \mathbf{W}_{yh} \cdot \mathbf{h}_t + \mathbf{b}_y$

unde:

- $\mathbf{W}_{hx}$ este matricea de greutăți între intrare și stare
- $\mathbf{W}_{hh}$ este matricea de greutăți recurente (de la stare anterioară la stare curentă)
- $\mathbf{W}_{yh}$ este matricea de greutăți între stare și ieșire
- $\mathbf{b}_h$, $\mathbf{b}_y$ sunt vectori bias

#### Vizualizare intuitivă

RNN-ul poate fi privit ca o rețea desfășurată în timp:

```
x₁ → [RNN] → h₁ → y₁  
x₂ → [RNN] → h₂ → y₂  
x₃ → [RNN] → h₃ → y₃  
...
```

La fiecare pas, se folosește starea precedentă $h_{t-1}$ pentru a calcula $h_t$.


#### Probleme ale RNN-urilor clasice

1. **Vanishing gradients**: gradientul devine foarte mic → învățarea pe termen lung devine imposibilă.
2. **Exploding gradients**: gradientul devine foarte mare → rețeaua devine instabilă.

Aceste probleme apar mai ales în secvențe lungi, unde informația trebuie „păstrată” pe mai mulți pași.


#### Soluții îmbunătățite: LSTM și GRU

Pentru a combate problemele RNN-urilor clasice, au fost dezvoltate:

- **LSTM (Long Short-Term Memory)**
- **GRU (Gated Recurrent Unit)**

Aceste arhitecturi folosesc **mecanisme de „porți”** pentru a controla mai eficient:

- ce informație se păstrează
- ce se uită
- ce se adaugă în starea internă

#### Exemple de aplicații ale RNN-urilor

- Generare de text caracter cu caracter
- Predicție cuvânt următor într-o propoziție
- Recunoaștere vorbire
- Analiză sentimente în texte
- Traducere automată

#### Avantaje ale RNN

- Procesare naturală a secvențelor
- Memorie internă – poate „aminti” informații anterioare

#### Limitări ale RNN

- Nu pot învăța eficient dependențe pe termen lung
- Timp de antrenare ridicat
- Performanță mai slabă decât Transformerii în NLP modern