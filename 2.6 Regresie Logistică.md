We have some data & a line:
![[Pasted image 20250609182336.png]]
X = weight of a new mouse
![[Pasted image 20250609182435.png]]
![[Pasted image 20250609182535.png]]
![[Pasted image 20250609182612.png]]
![[Pasted image 20250609182709.png]]
-> comparing the simple model to the complicated one tells us if we need to measure weight and blood volume to accurately predict size or if we get away with just weight.
LOGISTIC REGRESION

![[Pasted image 20250609182919.png]]
![[Pasted image 20250609182940.png]]
The curve goes from 0 to 1 that means the curve tells you the probability based on weight.


**Regresia logistică** este un algoritm de **învățare supravegheată** folosit pentru **clasificare**.  
Scopul este să învețe, din date etichetate, **probabilitatea** ca un exemplu nou să aparțină unei anumite clase.

Este numită „regresie” deoarece modelul se bazează pe o **funcție liniară** (ca în regresia liniară), dar ieșirea sa este trecută printr-o funcție specială (sigmoid), care o transformă într-o **probabilitate între 0 și 1**.


##### De ce nu folosim regresie liniară?

Dacă folosim regresie liniară pentru o variabilă de ieșire binară (0 sau 1), modelul poate produce valori ca 1.8 sau -0.3, care nu au sens ca „probabilități”.

Regresia logistică **corectează acest lucru** prin aplicarea funcției **sigmoid**, care:

- comprimă orice număr real într-un interval între 0 și 1
- poate fi interpretat ca o **probabilitate** ca exemplul să aparțină clasei 1

## Structura datelor

#### Date de intrare

Fiecare exemplu este reprezentat printr-un vector de caracteristici:

$x^{(i)} = [x_1^{(i)}, x_2^{(i)}, ..., x_d^{(i)}] \in \mathbb{R}^d$

Adică fiecare exemplu are $d$ caracteristici numerice (poate fi orice: înălțime, greutate, vârstă, etc.).

#### Etichete de ieșire

Pentru clasificare binară:

$y^{(i)} \in {0, 1}$

Adică avem doar două clase posibile. De exemplu:

- 1 = e-mail spam, 0 = e-mail normal
- 1 = pacient bolnav, 0 = pacient sănătos


### Modelul de bază

Modelul este o funcție liniară:

$f(x) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_dx_d$

sau, vectorial:

$f(x) = \boldsymbol{\beta}^T \mathbf{x}$

**Important:** această funcție **nu este ieșirea finală**. Ea produce un scor, un „z” care trebuie transformat într-o probabilitate.

### Funcția Sigmoid

Funcția sigmoid (numită și logistică) transformă scorul liniar $z$ într-o probabilitate:

$\text{Sigmoid}(z) = \frac{1}{1 + e^{-z}}$

Caracteristici:

- Dacă $z$ este foarte mare, atunci $\text{Sigmoid}(z) \to 1$
- Dacă $z$ este foarte mic, atunci $\text{Sigmoid}(z) \to 0$
- Dacă $z = 0$, atunci $\text{Sigmoid}(z) = 0.5$

Această funcție asigură că predicția noastră se află mereu între 0 și 1.


### Interpretarea ieșirii

Modelul returnează:

$\hat{y} = \text{Sigmoid}(f(x)) = P(y = 1 | x)$

Deci, ieșirea este **probabilitatea ca exemplul să aparțină clasei 1**.
### Prag de decizie (threshold)

Pentru a decide între clasa 0 și 1:

- Dacă $\hat{y} \geq 0.5$, prezicem clasa 1
- Dacă $\hat{y} < 0.5$, prezicem clasa 0

Notă: pragul de 0.5 este standard, dar poate fi modificat în funcție de aplicație (de exemplu, în medicină, putem vrea ca modelul să fie mai precaut și să clasifice drept „bolnav” și la 0.3).


### Funcția de cost: Binary Cross Entropy

Pentru a învăța coeficienții $\beta$, avem nevoie de o funcție care să penalizeze predicțiile greșite.

Aceasta este:

$J(\boldsymbol{\beta}) = -\frac{1}{m} \sum_{i=1}^{m} \left[y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)})\right]$

unde:

- $m$ este numărul total de exemple
- $\hat{y}^{(i)}$ este predicția modelului pentru exemplul $i$

Aceasta se numește **binary cross entropy loss** și are forma unei log-verosimilități negative.

### Antrenarea modelului – Gradient Descent

Scopul este să minimizăm funcția de cost $J(\boldsymbol{\beta})$ față de coeficienții $\beta$.

1. Se initializează:  
    $\beta_0, \beta_1, ..., \beta_d$ (aleator sau cu zero)
2. Se calculează predicția:  
    $\hat{y}^{(i)} = \text{Sigmoid}(\beta_0 + \beta_1 x_1^{(i)} + \dots + \beta_d x_d^{(i)})$
3. Se calculează eroarea:  
    $e^{(i)} = \hat{y}^{(i)} - y^{(i)}$
4. Se actualizează coeficienții:

$\beta_k := \beta_k - \alpha \cdot e^{(i)} \cdot x_k^{(i)}$  
pentru $k = 1, ..., d$

$\beta_0 := \beta_0 - \alpha \cdot e^{(i)}$  
(bias-ul nu se înmulțește cu $x$ pentru că e constanta 1)


#### Exemplu practic simplificat

**Scop**: prezicerea dacă un student va trece un examen.

Caracteristici:

- $x_1$ = ore de studiu
- $x_2$ = ore de somn

Date:

|Student|$x_1$|$x_2$|$y$|
|---|---|---|---|
|A|5|6|1|
|B|2|4|0|

Modelul:

$f(x) = \beta_0 + \beta_1 x_1 + \beta_2 x_2$

1. Calculezi $f(x)$
2. Aplici sigmoid: $\hat{y} = \text{Sigmoid}(f(x))$
3. Dacă $\hat{y} \geq 0.5$, prezici 1; altfel, 0
4. Actualizezi coeficienții folosind eroarea: $\hat{y} - y$


### Regresie Logistică vs Regresie Liniară

|Caracteristică|Regresie Liniară|Regresie Logistică|
|---|---|---|
|Scop|Predicție numerică|Clasificare|
|Ieșire|Orice număr real|Probabilitate (0–1)|
|Funcție finală|identitate|sigmoid|
|Funcție de cost|MSE|log loss|


## Aplicații frecvente

- Detectarea fraudelor bancare
- Clasificarea e-mailurilor (spam / non-spam)
- Diagnostic medical (boală / sănătos)
- Prezicerea comportamentului utilizatorilor (clic / nu clic)


