

**Regresia liniară** este o metodă statistică folosită pentru a modela relația dintre o **variabilă dependentă** (target) și una sau mai multe **variabile independente** (features). Scopul este de a găsi o funcție liniară care să prezică valoarea ieșirii în funcție de intrări.

- Dacă avem o singură variabilă independentă: **regresie liniară simplă**.
- Dacă avem mai multe variabile: **regresie liniară multiplă**.

## Forma generală

![[Pasted image 20250609161935.png]]

### Regresie liniară simplă (cu o singură caracteristică $x$):

$\hat{y} = w_0 + w_1 x$

Unde:

- $\hat{y}$ este valoarea prezisă,
- $x$ este caracteristica (feature),
- $w_0$ este interceptul (bias),
- $w_1$ este coeficientul de panta (weight).

### Regresie liniară multiplă (cu mai multe caracteristici):

$\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + \dots + w_n x_n = \mathbf{w}^T \mathbf{x}$

Reprezentare vectorială:

$\hat{y} = \mathbf{w}^T \mathbf{x}$

Unde:

- $\mathbf{x} = [1, x_1, x_2, \dots, x_n]^T$
- $\mathbf{w} = [w_0, w_1, w_2, \dots, w_n]^T$

#### Scopul antrenării

Antrenarea modelului înseamnă **găsirea coeficienților $\mathbf{w}$** astfel încât eroarea între predicții și valorile reale să fie minimă.


#### Funcția de cost (Eroarea pătratică medie - MSE)

Folosim **Mean Squared Error (MSE)** pentru a evalua performanța:

$J(\mathbf{w}) = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2 = \frac{1}{m} \sum_{i=1}^{m} (\mathbf{w}^T \mathbf{x}^{(i)} - y^{(i)})^2$

Unde:

- $m$ este numărul de exemple de antrenare,
- $\hat{y}^{(i)}$ este predicția pentru exemplul $i$,
- $y^{(i)}$ este valoarea reală.


#### Optimizare: Gradient Descent

Pentru a minimiza funcția de cost folosim **algoritmul Gradient Descent**:

$w_j := w_j - \alpha \cdot \frac{\partial J(\mathbf{w})}{\partial w_j}$

Pentru toate $j = 0, 1, \dots, n$, unde $\alpha$ este **rata de învățare** (learning rate).


#### Soluție închisă (Closed-form) – Formula normală

O altă metodă de a calcula coeficienții (fără iterații) este:

$\mathbf{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}$

Aceasta se folosește mai ales când setul de date nu este foarte mare.



### Avantaje:

- Simplu de implementat
- Interpretabil
- Rapid de antrenat

### Dezavantaje:

- Presupune relație liniară între variabile
- Sensibil la valori extreme (outliers)
- Nu funcționează bine cu date multicoliniare sau nelineare