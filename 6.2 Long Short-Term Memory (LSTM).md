**LSTM** este o variantă specială de **rețea neuronală recurentă (RNN)**, concepută pentru a învăța **dependențe pe termen lung** în date secvențiale, cum ar fi text, vorbire sau serii temporale.

Este capabilă să „țină minte” informații importante pe perioade mai lungi de timp și să ignore informațiile irelevante.


#### Motivul pentru care avem nevoie de LSTM

RNN-urile clasice suferă de problema **vanishing gradients** – pe măsură ce secvența devine mai lungă, gradientul devine tot mai mic, iar învățarea nu mai are loc eficient.

**LSTM** rezolvă această problemă printr-un **mecanism intern de control numit „celulă de memorie”**, care decide:

- ce informație să păstreze,
- ce să uite,
- ce să actualizeze.

#### Componentele unui LSTM

Fiecare „neuron” LSTM are patru componente principale, numite **porți**:

1. **Forget gate** – decide ce informație din starea precedentă să fie uitată.
2. **Input gate** – decide ce informație nouă se adaugă în celulă.
3. **Candidate memory** – o propunere de informație care ar putea fi adăugată în memorie.
4. **Output gate** – decide ce se transmite către ieșirea curentă.

#### Notare

- $\mathbf{x}_t$ = intrarea la momentul $t$
- $\mathbf{h}_t$ = stare ascunsă (hidden state) la momentul $t$
- $\mathbf{c}_t$ = stare internă a celulei (cell state)
- $\sigma$ = sigmoid
- $\tanh$ = funcția tangenta hiperbolică

#### Formule LSTM

1. **Forget gate**:
$\mathbf{f}_t = \sigma(\mathbf{W}_f \cdot [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_f)$

3. **Input gate**:
$\mathbf{i}_t = \sigma(\mathbf{W}_i \cdot [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_i)$

3. **Candidate memory**:
$\tilde{\mathbf{c}}_t = \tanh(\mathbf{W}_c \cdot [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_c)$

4. **Actualizare stare celulă**:
$\mathbf{c}_t = \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{c}}_t$

5. **Output gate**:
$\mathbf{o}_t = \sigma(\mathbf{W}_o \cdot [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_o)$

6. **Starea ascunsă actualizată**:
$\mathbf{h}_t = \mathbf{o}_t \odot \tanh(\mathbf{c}_t)$


#### Intuiție pas cu pas

1. **Forget gate**: decide ce părți din memoria veche $\mathbf{c}_{t-1}$ trebuie păstrate.
2. **Input gate + Candidate**: decide ce informații noi să fie adăugate.
3. **Update**: combină cele două pentru a obține noua memorie $\mathbf{c}_t$.
4. **Output gate**: decide ce se transmite mai departe ca ieșire (stare ascunsă $\mathbf{h}_t$).

#### Avantaje LSTM

- Ține minte informații relevante pe termen lung.
- Elimină zgomotul prin porți de uitare.
- Funcționează bine în sarcini secvențiale (traducere, predicție de text, etc).
#### Exemple de aplicații

- Traducere automată (ex: Google Translate)
- Recunoaștere vocală
- Generare de text
- Analiza sentimentelor
- Modelarea seriilor temporale (predicție bursieră)