
**PSO (Particle Swarm Optimization)** este un algoritm de optimizare inspirat de **comportamentul colectiv** al roiurilor (ex: stoluri de păsări, bancuri de pești), introdus de **Kennedy și Eberhart** în 1995.

Este un **algoritm evolutiv**, dar nu folosește selecție, crossover și mutație ca în algoritmii genetici. În schimb, particulele învață **colectiv**, adaptându-și pozițiile în funcție de **experiența proprie** și **a vecinilor**.

#### Intuiție

Imaginează-ți un **roi de păsări** care zboară într-un spațiu, încercând să găsească **zona cu cea mai mare hrană** (adică punctul cu cel mai bun fitness). Fiecare pasăre:

- Are o **poziție** și o **viteză**.
- Ține minte **cea mai bună poziție** unde a fost vreodată.
- Este influențată de cea mai bună poziție din roi.

#### Componentele principale

- $x_i(t)$ = **poziția** particulei $i$ la momentul $t$.
- $v_i(t)$ = **viteza** particulei $i$ la momentul $t$.
- $p_i$ = **cea mai bună poziție** găsită de particula $i$ (personal best).
- $g$ = **cea mai bună poziție globală** (global best) din tot roiul.

#### Etapele algoritmului

1. Inițializează particulele cu poziții și viteze aleatorii.
2. Pentru fiecare particulă:
    - Calculează fitness-ul.
    - Actualizează $p_i$ dacă s-a găsit o poziție mai bună.
3. Actualizează $g$ cu cea mai bună dintre toate $p_i$.
4. Actualizează viteza și poziția fiecărei particule:
    - **Viteză**:
        $v_i(t+1) = w \cdot v_i(t) + c_1 \cdot r_1 \cdot (p_i - x_i(t)) + c_2 \cdot r_2 \cdot (g - x_i(t))$
    - **Poziție**:
        $x_i(t+1) = x_i(t) + v_i(t+1)$

#### Parametri

- $w$ = **factor de inerție**: controlează influența vitezei anterioare (explorare globală)
- $c_1$ = coeficient de învățare **individuală** (cât de mult particula ține cont de propria experiență).
- $c_2$ = coeficient de învățare **colectivă** (cât de mult particula ține cont de roi).
- $r_1$, $r_2$ = numere aleatoare din $[0, 1]$ (introduc diversitate).

#### Interpretare intuitivă

- **$w \cdot v_i(t)$**: păstrează din mișcarea anterioară.
- **$(p_i - x_i(t))$**: atracție spre experiența proprie.
- **$(g - x_i(t))$**: atracție spre cei mai buni din grup.

#### Avantaje

- Algoritm **simplu** și **ușor de implementat**.
- Fără operatori genetici (se bazează pe actualizare vectorială).
- Performant pentru **probleme continue** și **optimizare numerică**.
#### Limitări

- Poate **converge prematur** la un optim local.
- Sensibil la alegerea parametrilor ($w$, $c_1$, $c_2$).
- Nu se potrivește direct problemelor discrete fără adaptări.

#### Unde se folosește PSO?

- Optimizarea funcțiilor reale (minimizare/maximizare).
- Probleme inginerești (parametrii optimi pentru sisteme).
- Antrenarea rețelelor neuronale (alegerea ponderilor).
- Probleme de planificare și alocare resurse.