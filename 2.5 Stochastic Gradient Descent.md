
-> folosit în special când sunt redundanțe în date.

![[Pasted image 20250609180416.png]]
![[Pasted image 20250609180436.png]]
![[Pasted image 20250609180535.png]]
Then we pick another random point and repeat the steps.

**Stochastic Gradient Descent** este o variantă a algoritmului Gradient Descent. În loc să folosească toate datele pentru a calcula direcția de minimizare (ca în Batch Gradient Descent), SGD actualizează coeficienții **după fiecare exemplu de antrenament**.

Termenul _stochastic_ înseamnă aleator — direcția în care modificăm parametrii conține o doză de incertitudine, dar în medie se apropie de minimum.

#### Intuiție

Imaginează-ți o persoană care coboară un deal cu ochii închiși. La fiecare pas, simte înclinarea doar dintr-un punct sub picioare și face un pas mic în acea direcție. Nu va merge în linie dreaptă spre vale, dar cu pași mici și multipli, în cele din urmă ajunge jos.

La fel și SGD: folosește doar o mică parte din informație (un exemplu) pentru a actualiza modelul, ceea ce îl face rapid, dar ușor oscilant.


### Formula de actualizare

Pentru fiecare exemplu $\left(\mathbf{x}^{(i)}, y^{(i)}\right)$ din setul de antrenament:

1. Se face predicția:  
    $\hat{y}^{(i)} = \mathbf{w}^T \mathbf{x}^{(i)}$
2. Se calculează eroarea:  
    $e^{(i)} = \hat{y}^{(i)} - y^{(i)}$
3. Se actualizează fiecare coeficient:  
    $w_j := w_j - \alpha \cdot e^{(i)} \cdot x_j^{(i)}$

unde:

- $w_j$ este coeficientul pentru caracteristica $j$,
- $\alpha$ este rata de învățare,
- $e^{(i)}$ este eroarea la exemplul $i$,
- $x_j^{(i)}$ este valoarea caracteristicii $j$ pentru exemplul $i$.


#### Comparație cu alte variante

|Tip algoritm|Date folosite per update|Viteză|Stabilitate|
|---|---|---|---|
|Batch GD|întregul set|lent|foarte stabil|
|SGD|un singur exemplu|rapid|oscilant|
|Mini-batch|un subset mic|echilibru între cele două||


**Avantaje**

- Funcționează foarte bine pe seturi **mari de date**.
- Este **rapid** deoarece nu trebuie să proceseze toate datele simultan.
- Poate **evita minimele locale** în funcții de cost complexe.

 **Dezavantaje**

- **Funcția de cost nu scade constant** — poate oscila.
- Alegerea unei **rate de învățare** $\alpha$ prea mari sau prea mici afectează performanța.
- Modelul poate avea nevoie de mai multe epoci pentru convergență.

### Tehnici de îmbunătățire

#### 1. Learning rate decay (scăderea ratei de învățare)

Se folosește o formulă de tipul:  
$\alpha_t = \frac{\alpha_0}{1 + k \cdot t}$  
unde $t$ este epoca curentă, $k$ este o constantă.

#### 2. Momentum

SGD cu momentum adaugă „inerție” în direcția mișcării:  
$v_j := \gamma v_j + \alpha \cdot \frac{\partial J^{(i)}(w)}{\partial w_j}$  
$w_j := w_j - v_j$  
unde $\gamma$ este coeficientul de momentum (ex. $0.9$).


**Când este util?**

- Când ai date mari și vrei un algoritm eficient.
- În antrenarea rețelelor neuronale (deep learning).
- Când spațiul de parametri este complex și are mai multe minime locale.