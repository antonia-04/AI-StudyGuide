### Softmax

**Softmax** este o funcție folosită frecvent în **stratul de ieșire** al rețelelor neuronale pentru **clasificare multiclasa** (când sunt mai mult de două clase).

Softmax transformă un vector de scoruri brute (logits) într-o **distribuție de probabilitate** — adică:

- Valorile ies între $0$ și $1$
- Suma tuturor ieșirilor este $1$
![[Pasted image 20250610211152.png]]
##### Formula:

Fie $z = [z_1, z_2, ..., z_K]$ — scorurile brute pentru cele $K$ clase. Atunci:
$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$

unde:

- $z_i$ este scorul pentru clasa $i$
- $\text{softmax}(z_i)$ este **probabilitatea** ca exemplul să aparțină clasei $i$

Softmax este **diferentiabil**, deci poate fi folosit în antrenare cu **cross-entropy loss**.

### Argmax

**Argmax** este o funcție care **alege indicele valorii maxime** dintr-un vector.

Dacă ai deja aplicat softmax și ai un vector de probabilități:

p=[0.1,0.7,0.2]p = [0.1, 0.7, 0.2]

atunci:

$\text{argmax}(p) = 1$

adică clasa cu probabilitatea cea mai mare.


### Diferențe esențiale:

| Funcție     | Ce face                              | Folosit pentru                  |
| ----------- | ------------------------------------ | ------------------------------- |
| **Softmax** | produce probabilități                | în timpul antrenării            |
| **Argmax**  | alege clasa cu probabilitatea maximă | în timpul testării / predicției |



##### Exemplu:

Scoruri brute: $z = [2.0,\ 1.0,\ 0.1]$

Aplicăm softmax:

- $e^{2.0} \approx 7.39$
- $e^{1.0} \approx 2.71$
- $e^{0.1} \approx 1.11$

Total: $7.39 + 2.71 + 1.11 \approx 11.21$

Probabilități:

- $p_1 = 7.39 / 11.21 \approx 0.659$
- $p_2 = 2.71 / 11.21 \approx 0.242$
- $p_3 = 1.11 / 11.21 \approx 0.099$

Deci:

- $\text{softmax}(z) = [0.659,\ 0.242,\ 0.099]$
- $\text{argmax} = 0$ → clasa 0 este cea mai probabilă
    