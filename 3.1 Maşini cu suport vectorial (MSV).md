
MSV se folosesc pentru clasificare. În 2D trag o linie care separă categoriile și în 3D trag un plan (se numesc margini).

![[Pasted image 20250609213319.png]]
Punctele care cad direct pe margine se numesc ***vectori suport*.**

MSV e învățare supervizată deoarece necesită un set de date cu label-uri și clasificate deja.

**Date non-liniar separabile**

În multe aplicații, datele nu pot fi separate de un hiperplan în spațiul original. O soluție comună este următoarea:
- Se **extind datele** prin adăugarea unor **caracteristici nelineare**, obținute din caracteristicile existente.
- Se **caută hiperplanul de separare** în acest nou spațiu de caracteristici (numit și feature space).
- Apoi se **proiectează înapoi** rezultatul în spațiul inițial pentru interpretare.
Această tehnică permite clasificarea datelor complexe, chiar dacă în spațiul original ele par imposibil de separat liniar.
Această proiecție este realizată eficient cu ajutorul unei **funcții kernel**, fără a calcula explicit coordonatele în spațiul extins — o abordare cunoscută sub numele de **"kernel trick"**.

![[Pasted image 20250609214245.png]]

Consideram un nou threshold care va fi la mijlocul dintre punctele extreme ale celor 2 categorii.

![[Pasted image 20250609214404.png]]

**terminology**: *margin* = the shortest distance between the observations and the threshold 

When we use the  threshold that gives us the largest margin to make classifications we are using a **Maximal Margin Classifier.**
dar nu va funcționa dacă training data arată așa și avem outliers:
![[Pasted image 20250609214834.png]]
![[Pasted image 20250609214856.png]]
![[Pasted image 20250609214913.png]]
Maximal Margin Classifiers are super sensitive to outliers in the training data.
**SOLUȚIE**:
To make a threshold that is not so sensitive to outliers we must allow misclassifications
![[Pasted image 20250609215053.png]]
then we will misclassify an observation but with new data it will be classified closed to the real output

![[Pasted image 20250609215200.png]]
![[Pasted image 20250609215234.png]]
and it performed poorly when we got new data(high variance)
![[Pasted image 20250609215459.png]]
it performed better when we got new data(low variance).
![[Pasted image 20250609215545.png]]
![[Pasted image 20250609215603.png]]
![[Pasted image 20250609215618.png]]
When we use a Soft Margin to determine the location of a threshold ![[Pasted image 20250609215720.png]]
![[Pasted image 20250609215753.png]]
![[Pasted image 20250609215821.png]]
![[Pasted image 20250609215833.png]]
![[Pasted image 20250609215846.png]]
Câteva puncte sunt în afara Soft Margin, câteva sunt misclassified.
![[Pasted image 20250609215942.png]]
#### Date 3D:
![[Pasted image 20250609220030.png]]
*Note*: the axis that age is on is supposed to represent depth, and the larger circles are to appear closer (thus younger)
![[Pasted image 20250609220143.png]]
Clasificăm noile observații în funcție de ce parte a planului se află, putem avea și în 4D(hyperplan)

![[Pasted image 20250610163036.png]]
red dots = patients not cured
green dots = patients that are cured

Now it doesn't matter where we put the threshold we would have a lot of misclassification so Suport Vector Classifiers don't perform well with this type of data

![[Pasted image 20250610163300.png]]

![[Pasted image 20250610163321.png]]

Transformam in date 2D:
![[Pasted image 20250610163403.png]]
![[Pasted image 20250610163423.png]]
![[Pasted image 20250610163742.png]]

![[Pasted image 20250610163820.png]]
![[Pasted image 20250610163833.png]]
![[Pasted image 20250610163845.png]]
**How do we transform the data?** 
![[Pasted image 20250610163946.png]]
![[Pasted image 20250610164003.png]]
![[Pasted image 20250610164017.png]]
![[Pasted image 20250610164025.png]]
![[Pasted image 20250610164034.png]]
![[Pasted image 20250610164048.png]]
![[Pasted image 20250610164057.png]]
![[Pasted image 20250610164105.png]]
We can find a good value for d with **Cross Validation**. Another very commonly used Kernel is Radial Kernel (Radial Basis Function Kernel) that finds Support Vector Classifiers in infinite dimensions.

Mașinile cu suport vectorial (SVM) sunt algoritmi de învățare supervizată folosiți pentru clasificare și regresie. Ele se bazează pe ideea de a găsi un hiperplan care separă clasele cu o margine maximă.

**Definire**

- Au fost dezvoltate de Vapnik în anii 1970 și popularizate după 1992.
- SVM sunt clasificatori liniari care determină un **hiperplan** de separare a clasei pozitive de cea negativă.
- Sunt susținute de o teorie matematică riguroasă și oferă performanțe bune pentru seturi de date mari (ex: text, imagini).

Un set de date este de forma $(x^d, t^d)$, unde:

- $x^d \in \mathbb{R}^m$ este vectorul de atribute,
- $t^d \in {-1, 1}$ este eticheta (1 pentru clasa pozitivă, -1 pentru clasa negativă).

Primele $n$ instanțe (cu etichete cunoscute) sunt folosite pentru antrenare, iar restul pentru testare.

Modelul SVM caută o funcție de forma:

$f(x) = \langle \mathbf{w}, \mathbf{x} \rangle + b$

unde $\mathbf{w}$ este vectorul de pondere, iar $b$ este bias-ul. Hiperplanul de decizie este definit de:

$\langle \mathbf{w}, \mathbf{x} \rangle + b = 0$

Clasificarea unui exemplu $x_i$ se face astfel:

- $\langle \mathbf{w}, \mathbf{x}_i \rangle + b \geq 0 \Rightarrow y_i = 1$
- $\langle \mathbf{w}, \mathbf{x}_i \rangle + b < 0 \Rightarrow y_i = -1$

#### Alegerea hiperplanului

Există mai multe hiperplane posibile. SVM alege acel hiperplan care maximizează **marginea** (distanța dintre cele mai apropiate puncte din fiecare clasă și hiperplan).

Aceasta minimizează eroarea de generalizare.
Pentru antrenare se poate folosi algoritmul SMO (Sequential Minimal Optimization).

#### Tipuri de probleme

- **Date liniar separabile**: există un hiperplan perfect care separă cele două clase (fără erori).
- **Date neliniar separabile**: se permit erori prin relaxarea constrângerilor; se introduce un coeficient de penalizare $C$.
- **Date non-liniar separabile**: se transformă spațiul de intrare într-un spațiu cu dimensiuni mai mari (feature space) cu ajutorul unei funcții _kernel_.
    

#### Kernele posibile

Clasice:
- **Polinomial**: $K(x_1, x_2) = (\langle x_1, x_2 \rangle + 1)^d$
- **RBF (Radial Basis Function)**: $K(x_1, x_2) = \exp(-\sigma |x_1 - x_2|^2)$

Kerneluri multiple
- Liniar: $K(x_1, x_2) = \sum w_i K_i(x_1, x_2)$
- Neliniare:
    - Fără coeficienți: $K = K_1 + K_2 \cdot \exp(K_3)$
    - Cu coeficienți: $K = K_1 + c_1 \cdot K_2 \cdot \exp(c_2 + K_3)$

### Configurarea unui SVM

#### Parametrii importanți:

- **C** (coeficientul de penalizare):
    - C mic: permite mai multe erori, antrenare mai lentă
    - C mare: penalizează puternic erorile, antrenare mai rapidă

#### Alegerea kernelului:

- Dacă $m \gg n$: kernel liniar (ex: $K(x_1, x_2) = x_1 \cdot x_2$)
- Dacă $m$ mare, $n$ mediu: kernel RBF, cu $\sigma$ legat de dispersia datelor
- Atributele trebuie **normalizate** (ex: interval [0,1])
- Dacă $m$ mic, $n$ mare: se pot adăuga atribute și folosi kernel liniar


### Clasificare multiclasa

SVM poate fi extins pentru mai mult de două clase prin strategia **One-vs-Rest** (antrenăm un SVM pentru fiecare clasă contra celorlalte).
![[Pasted image 20250609213036.png]]
#### SVM structurate

Sunt folosite în sarcini complexe cu ieșiri structurate:
- Intrări: orice tip (text, imagine, structură moleculară)
- Ieșiri: structuri (arbore sintactic, graf, semnal, propoziție etc.)

##### Aplicații:
- Procesare limbaj natural (traduceri, parsing)
- Bioinformatică (predicția de structuri sau funcții enzimatice)
- Procesare vorbire (recunoaștere vocală, TTS)
- Robotică (planificare de acțiuni)
##### Avantaje
- Pot lucra cu date diverse, fără a presupune o distribuție anume
- Kernelurile joacă rolul straturilor ascunse din RNA
- Oferă soluții **globale** într-un cadru **convex**
- Nu necesită alegerea explicită a dimensiunii modelului (vectorii suport definesc modelul)
- Rezistente la **overfitting** comparativ cu RNA
##### Dificultăți
- Se aplică pe atribute **numerice**
- Nativ doar pentru clasificare **binară**
- Necesită un background matematic solid
##### Tool-uri utile
- [LibSVM](http://www.csie.ntu.edu.tw/~cjlin/libsvm/)
- Weka (SMO)
- [SVMlight](http://svmlight.joachims.org/)
- [SVMtorch](http://www.torch.ch/)
- [support-vector-machines.org](http://www.support-vector-machines.org/)