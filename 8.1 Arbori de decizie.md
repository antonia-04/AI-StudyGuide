![[Pasted image 20250611213442.png]]

#### Ce sunt arborii de decizie?

Un **arbore de decizie** este un model de învățare supervizată folosit atât pentru **clasificare**, cât și pentru **regresie**.

Este o structură de tip **arbore binar**, în care:

- Fiecare **nod intern** reprezintă o întrebare despre o caracteristică (feature).
- Fiecare **ramură** este un rezultat al întrebării (de exemplu, "da" / "nu").
- Fiecare **frunză** conține o predicție (clasă sau valoare numerică).

#### Cum funcționează?

Modelul împarte datele în subseturi din ce în ce mai pure, punând întrebări de forma:

> "Caracteristica $x_i$ este mai mică decât un prag?"

Exemplu: dacă vrem să prezicem dacă o persoană va juca tenis, putem avea un arbore ca:

```
Temperatură > 30?
├── Da: Nu joacă
└── Nu:
    └── Umiditate > 70?
        ├── Da: Nu joacă
        └── Nu: Joacă
```


#### Algoritm de construire

Construirea unui arbore de decizie presupune parcurgerea următorilor pași:

1. Se alege caracteristica și pragul de tăiere care oferă cea mai bună separare.
2. Se împarte setul de date în două subseturi.
3. Se aplică recursiv același procedeu pe fiecare subset, până când:
    - Datele sunt perfect separate,
    - Sau nu mai sunt suficiente date,
    - Sau adâncimea maximă a fost atinsă.

#### Măsuri de puritate (pentru clasificare)

Pentru a decide ce împărțire este mai bună, folosim o **funcție de impuritate**:

##### 1. Entropia:

În contextul **arborilor de decizie (ex: ID3)**, **entropia** este o măsură numerică a **impurității** unui nod (adică cât de amestecate sunt clasele din acel nod).
**Intuiție:**
- Dacă într-un nod avem toate instanțele dintr-o singură clasă → entropia = 0 (pur, fără incertitudine)
- Dacă avem 50% clasa A, 50% clasa B → entropia = maximă (impur, incertitudine mare)

$H(D) = - \sum_{i=1}^{k} p_i \log_2 p_i$

unde $p_i$ este proporția de exemple din clasa $i$.

**Exemple:**
1. Set pur:
	1. 100% din date sunt din clasa A → p1=1p_1 = 1p1​=1, restul 0
	2. $\text{Entropie} = -1 \cdot \log_2(1) = 0$
	3. **Set complet pur** → entropie 0
2. Set echilibrat:
	- 50% A, 50% B →$ p_1 = p_2 = 0.5$
	- $\text{Entropie} = -0.5 \log_2(0.5) - 0.5 \log_2(0.5) = -0.5 \cdot (-1) \cdot 2 = 1$ 
	- **Impuritate maximă**
	- 
##### 2. Gini impurity:

$G(D) = 1 - \sum_{i=1}^{k} p_i^2$

#### Informația câștigată (Information Gain)

Măsoară cât de mult scade impuritatea după o împărțire:

IG(D,A)=H(D)−∑v∈valori(A)∣Dv∣∣D∣H(Dv)IG(D, A) = H(D) - \sum_{v \in \text{valori}(A)} \frac{|D_v|}{|D|} H(D_v)

unde:

- $D$ este setul curent de date,
- $D_v$ este subsetul pentru valoarea $v$ a caracteristicii $A$.

Se alege caracteristica $A$ care maximizează $IG$.

#### Cum e folosită entropia în arbori?

- La fiecare pas, algoritmul **alege atributul care reduce cel mai mult entropia** (adică impuritatea).

#### Suprapotrivirea (Overfitting)

Arborii de decizie compleți pot învăța **zgomotul din date**, nu doar regulile reale.

**Soluții:**

- Limitarea adâncimii arborelui,
- Pruning (tăierea ramurilor inutile),
- Alegerea unui număr minim de exemple într-un nod.

#### Avantaje

- Ușor de interpretat,
- Nu necesită scalarea datelor,
- Pot trata date categorice și numerice.


#### Dezavantaje

- Prea sensibili la datele de antrenament,
- Pot fi instabili la mici modificări ale datelor (soluții: Random Forest),
- Suferă ușor de overfitting.

#### Vizualizare utilă

![Exemplu arbore de decizie](https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png)



